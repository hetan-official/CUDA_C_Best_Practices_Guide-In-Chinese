CUDA编程涉及到在两种不同的平台上同时执行代码：一个含一块或多块CPU的*host*系统和一个含有一块或几块支持CUDA编程的NVIDIA GPU的*device*系统。

虽然NVIDIA GPU尝尝与图形化相关联，但是其本身也是支持成千上万个轻量化线程并行执行的强大的计算卡。这种能力使得该卡适合那些需要并行执行的计算场景。

然而，*device*系统与*host*系统在设计思路上大不相同。因此，要想充分且高效地发挥CUDA应用的性能，理解他们的差异特别重要。

## 2.1 Deifferences between Host and Device
差异主要体现在线程资源和物理存储的设计上：

### 2.1.1 线程资源
host系统上的Execution pipelines仅支持有限几个并行线程。配备2个32个核心处理器仅可供64个线程并行执行（如果CPUs支持simultaneous multithreading的话，也可支持少数几倍于64个线程的并行计算）。作为对比，在支持CUDA设备上最小可并行单元的线程数量即为32个线程（称为线程*warp*）。现代NVIDIA GPUs支持一个SM（详情见 *《CUDA C++ Programming Guide》* Features and Specifications章节）上至多2048个线程同时执行，而一个GPU上则至多支持80个SMs，这意味着GPU上支持超过16000个线程同时执行。

### 2.1.2 线程
CPU上的线程通常都是重型线程。操作系统不得不通过频繁切换CPU运行上下文及其对应线程来提供多线程能力。因此，两个线程相关的上下文切换非常慢，因此开销很大。与之相反，GPUs上的线程非常轻量化。典型系统下，成千上万个线程成队工作(每32个线程组成为一个warp)。如果GPU中的一个线程warp处于wait状态，它可以很轻易地就切换到另一个warp的执行上。这是因为所有处于active状态的线程都会单独分配寄存器，所以当切换到其他GPU线程时无需对寄存器或者另外的状态量进行切换。每个线程所分配的资源直到线程完成执行之前均不会被释放。总而言之，CPU的核心设计思路是围绕有限的几个、数十个或者上百个线程的延迟最小化实现高性能，而GPUs则是通过大规模的线程并行，通常是成千上万个线程的并行，实现吞吐量的最大化来实现高性能。

### 2.1.3 RAM
host和device系统各有其独特的[附加物理存储](https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-best-practices-guide/index.html#fnsrc_1)。由于host内存和device内存并非直接连通，所以，正如[What Runs on a CUDA-Enabled Device?]()章节介绍的那样，host的内存和device的内存之间根据需要将不得不时常相互通信。

以上便是在CPU为主的host和GPU为主的device之间有关并行编程的主要差异，当然两者之间也并非不存在其他差异了，我们将在文档中提及之时再详细介绍。当开发在此类异构系统上运行的应用程序时，建议牢记host和device在硬件上的这些差异。如此，根据任务属性的特点，将适合串行执行的任务分配给host负责，同时将适合并行执行的任务分配给device，以最大化利用每个处理单元。

## 2.2 What Runs on a CUDA-Enabled Device?
当在规划应用程序的哪些部分可以被放到device上执行时应该考虑以下几点：

- 对于device而言，最适宜的是那些成千上万的数据元素可以被同时并行计算的任务。换言之，是那些牵涉到大量数据集(如矩阵)，规模达到成千上万，甚至数百万的数据同时执行相同操作的任务。对于使用CUDA获得良好性能来说，这一点非常重要：并发的线程必须对大量的数据有需求（通常是数千乃至数万）。能做到这一点，正是源于CUDA编程模型中具有的前文提到的万级的轻量级线程模型。
  
- 为了应用CUDA，数据必须从host端被传输到device端。从性能的角度来说，该传输操作开销巨大，因此有必要尽量减少此该行为。参考[Data Transfer Between Host and Device](),这些开销对以下方面均有影响：
  - 操作的复杂程度应该合理考虑将数据从device拷入拷出的开销。那些仅仅为了少数几个线程进行数据传输的代码对于程序性能来说并无益处。相反，理想的情况是众多的线程执行着大量的工作。例如，将两个矩阵从host传输到device端只进行一个矩阵加操作，随后将结果传输回host端的这样一个任务在性能上并不会提升太多。这个任务的问题是：对传输的数据所进行的算术操作数量太少了！这里我们简单分析一下，假设矩阵的size是NxN，那么这个任务中总共会对N^2个元素进行算术操作（矩阵相加）和3N^2个元素传输操作。因此算术操作和数据传输操作之比为1:3或者O(1).需要指出的是，当这个比值越高时，性能提升效果越明显。再比如说，对于相同规模的矩阵，进行multiply-add的操作，其元素操作次数将来到N^3，此时算术操作和传输操作的比值为O(N)，在这种情况下，矩阵的维度N越大，性能收益也就越大。操作的类型同样也是影响因素，加法的复杂度与诸如三角函数的是不同的。因此，在确定操作到底应该放到host还是device上执行时，充分评估数据在host和device之间的传输开销十分重要。
  - 数据应在device上驻留尽量长的时间。主要是数据传输应该最小化的要求，比如说，当应用程序启动多个kernel函数对相同的数据进行操作时，在kernel函数调用期间数据一直驻留在device端要好于将一个kernel函数计算的结果立马传回host端随后又传回device端并继续操作。因此，对于前文提到的矩阵相加的例子中，被加和的矩阵要么可以是之前计算的某些结果数据，又或者是加和的结果是后续一些计算需要用到的数据，矩阵加和都应该仅仅是仅牵涉到device端而不是涉及host和device端数据传输的操作。哪怕说，矩阵加和中的某一个环节在CPU上执行会更快一些，又或者在GPU上执行某些步骤的kernel函数执行速度更慢，对比来看，避免host和device间的不必要的数据传输仍然是综合效果更好的方案。[Data Transfer Between Host and Device]()章节中将会围绕这个话题详细论述，包括但不限于对host和device之间以及device内部的带宽度量技术。
 
- 为了获得最佳的性能效果，device上执行的相邻线程在访问的地址空间上应该具有一定的连续性。某些内存访问模式可以使得在硬件层面上对许多数据的读写合并为一个访存事务。对于GPUs而言，那些分散以致于无法合并访存的数据，以及那些缺乏局部性以致于无法有效利用L1或纹理缓存的数据，GPU并行计算加速效果往往欠佳。一个典型的例子便是完全随机的显存访存模式，大部分情况下，都应尽可能地避免出现。原因是，无论哪一代GPU架构，与峰值访存带宽利用率相比，这样随机访问的访存模式对带宽的利用率都很低。不过，相较于依赖于缓存的框架，比如CPU，像GPU这样的延迟隐藏实现方式在完全随机的内存访问模式上仍然会有更好的表现。
