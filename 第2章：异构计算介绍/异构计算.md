CUDA编程涉及到在两种不同的平台上同时执行代码：一个含一块或多块CPU的*host*系统和一个含有一块或几块支持CUDA编程的NVIDIA GPU的*device*系统。

虽然NVIDIA GPU尝尝与图形化相关联，但是其本身也是支持成千上万个轻量化线程并行执行的强大的计算卡。这种能力使得该卡适合那些需要并行执行的计算场景。

然而，*device*系统与*host*系统在设计思路上大不相同。因此，要想充分且高效地发挥CUDA应用的性能，理解他们的差异特别重要。

## 2.1 Host 和 Device 的区别
差异主要体现在线程资源和物理存储的设计上：

### 2.1.1 线程资源
host系统上的Execution pipelines仅支持有限几个并行线程。配备2个32个核心处理器仅可供64个线程并行执行（如果CPUs支持simultaneous multithreading的话，也可支持少数几倍于64个线程的并行计算）。作为对比，在支持CUDA设备上最小可并行单元的线程数量即为32个线程（称为线程*warp*）。现代NVIDIA GPUs支持一个SM（详情见 *《CUDA C++ Programming Guide》* Features and Specifications章节）上至多2048个线程同时执行，而一个GPU上则至多支持80个SMs，这意味着GPU上支持超过16000个线程同时执行。

### 2.1.2 线程
CPU上的线程通常都是重型线程。操作系统不得不通过频繁切换CPU运行上下文及其对应线程来提供多线程能力。因此，两个线程相关的上下文切换非常慢，因此开销很大。与之相反，GPUs上的线程非常轻量化。典型系统下，成千上万个线程成队工作(每32个线程组成为一个warp)。如果GPU中的一个线程warp处于wait状态，它可以很轻易地就切换到另一个warp的执行上。这是因为所有处于active状态的线程都会单独分配寄存器，所以当切换到其他GPU线程时无需对寄存器或者另外的状态量进行切换。每个线程所分配的资源直到线程完成执行之前均不会被释放。总而言之，CPU的核心设计思路是围绕有限的几个、数十个或者上百个线程的延迟最小化实现高性能，而GPUs则是通过大规模的线程并行，通常是成千上万个线程的并行，实现吞吐量的最大化来实现高性能。

### 2.1.3 RAM
host和device系统各有其独特的[附加物理存储](https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-best-practices-guide/index.html#fnsrc_1)。